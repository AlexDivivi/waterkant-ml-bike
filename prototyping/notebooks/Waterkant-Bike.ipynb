{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Volume Prediction: Outsmarting Bike Sharing with Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/ac-logo.png\" style=\"height: 150px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About me:\n",
    "Alex Divivi\n",
    "\n",
    "alex.divivi@artificial-connect.com\n",
    "\n",
    "Data Scientist and Developer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What even is Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../assets/machine-learning1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../assets/regression-ml-requirements.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference between ML and DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../assets/main-qimg-6c1dc5666bd31bf16120d332957b4059.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](../assets/crisp-dm-4-problems-fig1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's apply Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the data here: https://drive.google.com/file/d/14IeffIWwjWnWkOQy5vG33kuVLYuuq8Ni/view?usp=sharing\n",
    "\n",
    "Place the .csv in prototyping/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical step in Machine Learning is preparing the data correctly. Variables on different scales make it difficult for the model to efficiently learn the correct weights. Below, I've written the code to load and prepare the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/hour.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- instant: record index\n",
    "- dteday : date\n",
    "- season : season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : weather day is holiday or not\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit : \n",
    "- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has the number of riders for each hour of each day from January 1 2011 to December 31 2012. The number of riders is split between casual and registered, summed up in the cnt column. You can see the first few rows of the data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a plot showing the number of bike riders over the first 10 days in the data set. You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:24*10].plot(x='dteday', y='cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to get_dummies()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = # TODO: Implement categorical variable names as array containing strings\n",
    "for each in categorical_variables:\n",
    "    cat = pd.get_dummies(data[each], prefix=each, drop_first=False)\n",
    "    data = pd.concat([data, cat], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model to learn something, we'll need to get rid of the old columns containing the categorical variables, the dteday (object) and the instant (index).\n",
    "\n",
    "Do we also need to drop 'casual' and 'registered'? Try to explain why or why not.\n",
    "You may try to keep / drop 'atemp' and 'workingday' and see how the model reacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fields_to_drop = # TODO: Implement variable names to drop (remove) from the dataset as array containing strings\n",
    "encoded_data = data.drop(fields_to_drop, axis=1)\n",
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll save the last 31 days of the data to use as a test set after we've trained the model. We'll use this set to make predictions and compare them with the actual number of riders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last 31 days for final testing\n",
    "test_data = encoded_data[-31*24:]\n",
    "train_data = encoded_data[:-31*24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data in X (features) and y (target)\n",
    "X, X_test = np.array(train_data.drop(['cnt'], axis=1)), np.array(test_data.drop(['cnt'], axis=1))\n",
    "y, y_test = np.array(train_data['cnt']), np.array(test_data['cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler = preprocessing.MinMaxScaler()\n",
    "y = target_scaler.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the data into two sets, one for training and one for validating as the model is being trained. It's important to split the data randomly so all cases are represented in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y in a train\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Regression Model with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\tilde{f}(\\boldsymbol{x}) = \\boldsymbol{w}^T \\cdot\\boldsymbol{x} + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PyTorch and Tensors\n",
    "device = torch.device(\"cpu\")\n",
    "dtype = torch.float\n",
    "X_train = torch.as_tensor(X_train, device=device, dtype=dtype)\n",
    "X_val = torch.as_tensor(X_val, device=device, dtype=dtype)\n",
    "y_train = torch.as_tensor(y_train, device=device, dtype=dtype)\n",
    "y_val = torch.as_tensor(y_val, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the number of epochs\n",
    "This is the number of times the dataset will pass through the model, each time updating the weights. As the number of epochs increases, the model becomes better and better at predicting the targets in the training set. However, it can become too specific to the training set and will fail to generalize to the validation set. This is called overfitting. You'll need to choose enough epochs to train the model well but not too many or you'll be overfitting.\n",
    "\n",
    "### Choose the learning rate\n",
    "This scales the size of weight updates. If this is too big, the weights tend to explode and the model fails to fit the data. A good choice to start at is 0.01. If the model has problems fitting the data, try reducing the learning rate. Note that the lower the learning rate, the smaller the steps are in the weight updates and the longer it takes for the model to converge.\n",
    "\n",
    "### Choose a batch size\n",
    "This defines the number of samples that will be propagated through the model. Remember: After each propagation the weights will be updated. For example: A batch size of 32 will update the weights after 32 samples have passed the model. Defining a batch size helps the model to converge much faster. Common values are 16, 32, 64 and 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyperparameter\n",
    "epoch = \n",
    "learning_rate = \n",
    "batch_size = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = nn.Sequential(\n",
    "     nn.Linear(X_train.shape[1], 1),\n",
    "     nn.Sigmoid())\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "loss_hist = []\n",
    "loss_hist_val = []\n",
    "ix = int(epoch / 2)\n",
    "\n",
    "for t in range(epoch):\n",
    "    for batch in range(int(X_train.shape[0]/batch_size)):\n",
    "        \n",
    "        batch_x = X_train[batch * batch_size : (batch + 1) * batch_size, :]\n",
    "        batch_y = y_train[batch * batch_size : (batch + 1) * batch_size, :]\n",
    "        \n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_outputs = model(X_val)\n",
    "        loss_v = criterion(val_outputs, y_val)\n",
    "\n",
    "    # Save and print Error all ix iterations\n",
    "    if t % ix == 0:\n",
    "        loss_hist.append(loss.item())\n",
    "        loss_hist_val.append(loss_v.item())\n",
    "        print(t, f'Error: {loss.item()}, Val_Error: {loss_v.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\lVert  \\tilde{f}(\\boldsymbol{x}) - y \\rVert_2^2 = \\frac{1}{N}\\sum\\left(\\tilde{f}(\\boldsymbol{x}) - y \\right)^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you see a plot of the Mean Squared Error of the model. While training the model the Error / Loss should steadily decrease. Note that Traning Loss and Validation Loss should be close together, otherwise your model might be underfitting or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist, label='Training loss')\n",
    "plt.plot(loss_hist_val, label='Validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, use the test data to check that model is accurately making predictions. If your predictions don't match the data, try adjusting the hyperparameters and / or experiment with the features. (You may also just upgrade to a Neural Network...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.as_tensor(X_test, device=device, dtype=dtype)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "predictions = model(X_test)\n",
    "ax.plot(target_scaler.inverse_transform(predictions.detach().numpy()), label='Prediction')\n",
    "ax.plot(y_test, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(data.loc[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe the perfomance of the model? Does it show weaknesses? If yes, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus Question: What do we need to do to upgrade our simple Linear Regression Model to a Neural Network (ANN)? \n",
    "How about a Deep Neural Network (DNN), Deep Learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
